import re
from typing import Dict, List, Optional

import httpx
from bs4 import BeautifulSoup

from school_bot.config import SCHOOL_URL


async def parse_school_info() -> Dict[str, Optional[str]]:
    """–¢–æ—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —à–∫–æ–ª–µ –¥–ª—è –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞"""
    default_info = {
        'name': None,
        'address': None,
        'director': None,
        'phones': None,
        'email': None,
        'description': None
    }
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(SCHOOL_URL)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            info = default_info.copy()

            # 1. –ü–∞—Ä—Å–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ —à–∫–æ–ª—ã (–±–µ—Ä–µ–º —Ç—Ä–µ—Ç–∏–π h2 —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º –∫–ª–∞—Å—Å–æ–º)
            name_tags = soup.find_all('h2', class_='name tpl-text-header2')
            if len(name_tags) >= 3:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 3 —ç–ª–µ–º–µ–Ω—Ç–∞
                name_tag = name_tags[2]  # –ë–µ—Ä–µ–º —Ç—Ä–µ—Ç–∏–π —ç–ª–µ–º–µ–Ω—Ç (–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å 0)
                name = name_tag.get_text(strip=True).replace('&quot;', '"')
                info['name'] = name.split(':')[0].strip()
            
            # 2. –ü–∞—Ä—Å–∏–º –∞–¥—Ä–µ—Å –∏–∑ object-index-text
            object_index = soup.find('div', class_='object-index-text')
            if object_index:
                address_tag = object_index.find('div', class_='address')
                if address_tag:
                    info['address'] = address_tag.get_text(strip=True)

            # 3. –ü–∞—Ä—Å–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ
            description_tag = soup.find('article', class_='tpl-text-default')
            if description_tag:
                description = description_tag.find('p').get_text(strip=True)
                info['description'] = description.replace('&quot;', '"').replace('  ', ' ')

            # 4. –ü–∞—Ä—Å–∏–º –∫–æ–Ω—Ç–∞–∫—Ç—ã (–æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
            contact_block = soup.find('div', class_='tpl-component-gw-staff')
            if contact_block:
                # –î–∏—Ä–µ–∫—Ç–æ—Ä
                director_tag = contact_block.find('a', attrs={'title': True})
                if director_tag:
                    director_name = director_tag['title'].strip()
                    director_position = contact_block.find('div', class_='contacts-object-info-subname')
                    position = director_position.get_text(strip=True) if director_position else "–î–∏—Ä–µ–∫—Ç–æ—Ä"
                    info['director'] = f"{position}: {director_name}"

                # –¢–µ–ª–µ—Ñ–æ–Ω—ã
                phone_header = contact_block.find('div', class_='tpl-text-header6', string=lambda t: '–¢–µ–ª–µ—Ñ–æ–Ω' in str(t))
                if phone_header:
                    phone_div = phone_header.find_next('div', class_='tpl-text-default-paragraph')
                    if phone_div:
                        info['phones'] = phone_div.get_text(strip=True).replace(' - ', '-')

                # Email
                email_header = contact_block.find('div', class_='tpl-text-header6', string=lambda t: '–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –ø–æ—á—Ç–∞' in str(t))
                if email_header:
                    email_div = email_header.find_next('div', class_='tpl-text-default-paragraph')
                    if email_div:
                        info['email'] = email_div.get_text(strip=True)

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–∞–∫—Ç—ã
            contacts = []
            if info['phones']:
                contacts.append(f"üìû –¢–µ–ª–µ—Ñ–æ–Ω—ã: {info['phones']}")
            if info['email']:
                contacts.append(f"üìß Email: {info['email']}")
            if info['address']:
                contacts.append(f"üìç –ê–¥—Ä–µ—Å: {info['address']}")
            
            info['contacts'] = "\n".join(contacts) if contacts else None

            return info

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        return default_info


async def parse_school_schedule() -> List[Dict[str, str]]:
    """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ PDF —Å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º —Å —Å–∞–π—Ç–∞ —à–∫–æ–ª—ã"""
    try:
        async with httpx.AsyncClient(timeout=20.0) as client:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º
            response = await client.get(f"{SCHOOL_URL}glavnoe/raspisanie/")
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            schedules = []
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –±–ª–æ–∫–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            doc_items = soup.find_all('div', class_='document-object-item')
            
            for item in doc_items:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
                name_tag = item.find('div', class_='document-caption')
                name = name_tag.get_text(strip=True).replace('&quot;', '"') if name_tag else "–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ"
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ PDF
                download_link = item.find('a', class_='document-download')
                if download_link and download_link.get('href'):
                    pdf_url = download_link['href']
                    # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è, –¥–µ–ª–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω–æ–π
                    if not pdf_url.startswith('http'):
                        pdf_url = f"{SCHOOL_URL.rstrip('/')}{pdf_url}"
                    
                    schedules.append({
                        'name': name,
                        'url': pdf_url
                    })
            
            return schedules

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è: {e}")
        return []